% Describe the data preprocessing steps
% Include:
% - Image resizing
% - Normalization
% - Any other preprocessing steps

\subsection{Preprocessing}

For preprocessing, I use the following steps:
\begin{enumerate}
    \item Resize the image to 256x256
    \item Randomly crop the image to 256x256 or rotate the image by at most 30 degrees
    \item Randomly flip the image horizontally or vertically
    \item Randomly adjust the brightness or gamma of the image
    \item Randomly add Gaussian noise or blur to the image
    \item Normalize the image with the mean and standard deviation of the ImageNet dataset
\end{enumerate}

and implement it in the \lstinline{load_dataset} function using the functions from \lstinline{albumentations} library.

\begin{lstlisting}[language=Python, caption=oxford\_pet.py: load\_dataset, label=lst:load_dataset]
A.Compose([
    A.Resize(256, 256),
    A.OneOf([
        A.RandomResizedCrop((256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.33), p=1.0),
        A.Rotate(limit=30, p=1.0),
    ], p=0.5),
    A.OneOf([
        A.HorizontalFlip(p=1.0),
        A.VerticalFlip(p=1.0),
    ], p=0.5),
    A.OneOf([
        A.RandomBrightnessContrast(p=1.0),
        A.RandomGamma(p=1.0),
    ], p=0.3),
    A.OneOf([
        A.GaussNoise(p=1.0),
        A.GaussianBlur(p=1.0),
    ], p=0.2),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])
\end{lstlisting}

\subsection{What makes the preprocessing method unique}
I choose some transformations of geometric, photometric, and noise to augment the data. But if we impose too many transformations, the image would be too different from the original image, and the model would not be able to learn the features.

So, I use the \lstinline{OneOf} function to choose one of the transformations to apply to the image. And use the parameter $p$ to control the probability of each transformation, prevent the image from being too different.

By this method, the model can learn from moderately augmented data, and the performance is better than using only the original data.
